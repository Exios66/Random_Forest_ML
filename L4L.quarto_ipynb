{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Random Forest Algorithms: Applications to Large-Scale Datasets\"\n",
        "subtitle: \"A Comprehensive Analysis of Ensemble Learning Methods\"\n",
        "author: \"Jack J. Burleson\"\n",
        "date: today\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    toc-location: left\n",
        "    code-fold: show\n",
        "    code-tools: true\n",
        "    theme: cosmo\n",
        "    include-in-header: |\n",
        "      <style>\n",
        "        /* CSS Variables for Theme */\n",
        "        :root {\n",
        "          --bg-primary: #ffffff;\n",
        "          --bg-secondary: #f8f9fa;\n",
        "          --text-primary: #333333;\n",
        "          --text-secondary: #666666;\n",
        "          --border-color: #e0e0e0;\n",
        "          --code-bg: #f5f5f5;\n",
        "          --header-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "          --shadow: rgba(0,0,0,0.1);\n",
        "        }\n",
        "        \n",
        "        [data-theme=\"dark\"] {\n",
        "          --bg-primary: #1a1a1a;\n",
        "          --bg-secondary: #2d2d2d;\n",
        "          --text-primary: #e0e0e0;\n",
        "          --text-secondary: #b0b0b0;\n",
        "          --border-color: #404040;\n",
        "          --code-bg: #2a2a2a;\n",
        "          --header-gradient: linear-gradient(135deg, #4a5568 0%, #2d3748 100%);\n",
        "          --shadow: rgba(0,0,0,0.5);\n",
        "        }\n",
        "        \n",
        "        /* Apply theme to body and main content */\n",
        "        body {\n",
        "          margin-top: 0;\n",
        "          background-color: var(--bg-primary);\n",
        "          color: var(--text-primary);\n",
        "          transition: background-color 0.3s ease, color 0.3s ease;\n",
        "        }\n",
        "        \n",
        "        /* Theme toggle button */\n",
        "        .theme-toggle {\n",
        "          background: rgba(255, 255, 255, 0.2);\n",
        "          border: 2px solid rgba(255, 255, 255, 0.3);\n",
        "          color: white;\n",
        "          padding: 8px 16px;\n",
        "          border-radius: 20px;\n",
        "          cursor: pointer;\n",
        "          font-size: 14px;\n",
        "          font-weight: 600;\n",
        "          transition: all 0.3s ease;\n",
        "          display: inline-flex;\n",
        "          align-items: center;\n",
        "          gap: 8px;\n",
        "          margin-left: 20px;\n",
        "        }\n",
        "        \n",
        "        .theme-toggle:hover {\n",
        "          background: rgba(255, 255, 255, 0.3);\n",
        "          border-color: rgba(255, 255, 255, 0.5);\n",
        "        }\n",
        "        \n",
        "        .theme-toggle svg {\n",
        "          width: 18px;\n",
        "          height: 18px;\n",
        "        }\n",
        "        \n",
        "        /* Header styles */\n",
        "        .custom-header {\n",
        "          background: var(--header-gradient);\n",
        "          color: white;\n",
        "          padding: 15px 20px;\n",
        "          text-align: center;\n",
        "          box-shadow: 0 2px 10px var(--shadow);\n",
        "          position: sticky;\n",
        "          top: 0;\n",
        "          z-index: 1000;\n",
        "          margin-bottom: 20px;\n",
        "        }\n",
        "        \n",
        "        .custom-header .header-content {\n",
        "          max-width: 1200px;\n",
        "          margin: 0 auto;\n",
        "          display: flex;\n",
        "          justify-content: center;\n",
        "          align-items: center;\n",
        "          flex-wrap: wrap;\n",
        "          gap: 15px;\n",
        "        }\n",
        "        \n",
        "        .custom-header a {\n",
        "          color: white;\n",
        "          text-decoration: none;\n",
        "          font-weight: 600;\n",
        "          margin: 0 15px;\n",
        "          transition: opacity 0.3s ease;\n",
        "        }\n",
        "        \n",
        "        .custom-header a:hover {\n",
        "          opacity: 0.8;\n",
        "          text-decoration: underline;\n",
        "        }\n",
        "        \n",
        "        .custom-header .github-link {\n",
        "          display: inline-flex;\n",
        "          align-items: center;\n",
        "          gap: 8px;\n",
        "        }\n",
        "        \n",
        "        /* Footer styles */\n",
        "        .custom-footer {\n",
        "          background: var(--header-gradient);\n",
        "          color: white;\n",
        "          padding: 25px 20px;\n",
        "          text-align: center;\n",
        "          margin-top: 50px;\n",
        "          box-shadow: 0 -2px 10px var(--shadow);\n",
        "        }\n",
        "        \n",
        "        .custom-footer a {\n",
        "          color: white;\n",
        "          text-decoration: none;\n",
        "          font-weight: 600;\n",
        "          margin: 0 15px;\n",
        "          transition: opacity 0.3s ease;\n",
        "        }\n",
        "        \n",
        "        .custom-footer a:hover {\n",
        "          opacity: 0.8;\n",
        "          text-decoration: underline;\n",
        "        }\n",
        "        \n",
        "        .custom-footer .footer-content {\n",
        "          max-width: 1200px;\n",
        "          margin: 0 auto;\n",
        "        }\n",
        "        \n",
        "        .custom-footer .footer-links {\n",
        "          margin: 15px 0;\n",
        "        }\n",
        "        \n",
        "        /* Dark theme adjustments for Quarto content */\n",
        "        [data-theme=\"dark\"] .quarto-title-block,\n",
        "        [data-theme=\"dark\"] .quarto-title {\n",
        "          color: var(--text-primary);\n",
        "        }\n",
        "        \n",
        "        [data-theme=\"dark\"] pre,\n",
        "        [data-theme=\"dark\"] code {\n",
        "          background-color: var(--code-bg);\n",
        "          color: var(--text-primary);\n",
        "        }\n",
        "        \n",
        "        [data-theme=\"dark\"] .cell-output {\n",
        "          background-color: var(--bg-secondary);\n",
        "        }\n",
        "        \n",
        "        [data-theme=\"dark\"] table {\n",
        "          border-color: var(--border-color);\n",
        "        }\n",
        "        \n",
        "        [data-theme=\"dark\"] .table {\n",
        "          color: var(--text-primary);\n",
        "        }\n",
        "      </style>\n",
        "      <div class=\"custom-header\">\n",
        "        <div class=\"header-content\">\n",
        "          <a href=\"https://github.com/Exios66\" class=\"github-link\" target=\"_blank\" rel=\"noopener\">\n",
        "            <svg width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"currentColor\" style=\"vertical-align: middle;\">\n",
        "              <path d=\"M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z\"/>\n",
        "            </svg>\n",
        "            GitHub: Exios66\n",
        "          </a>\n",
        "          <button class=\"theme-toggle\" id=\"theme-toggle\" aria-label=\"Toggle theme\">\n",
        "            <svg id=\"theme-icon\" width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\">\n",
        "              <circle cx=\"12\" cy=\"12\" r=\"5\"></circle>\n",
        "              <line x1=\"12\" y1=\"1\" x2=\"12\" y2=\"3\"></line>\n",
        "              <line x1=\"12\" y1=\"21\" x2=\"12\" y2=\"23\"></line>\n",
        "              <line x1=\"4.22\" y1=\"4.22\" x2=\"5.64\" y2=\"5.64\"></line>\n",
        "              <line x1=\"18.36\" y1=\"18.36\" x2=\"19.78\" y2=\"19.78\"></line>\n",
        "              <line x1=\"1\" y1=\"12\" x2=\"3\" y2=\"12\"></line>\n",
        "              <line x1=\"21\" y1=\"12\" x2=\"23\" y2=\"12\"></line>\n",
        "              <line x1=\"4.22\" y1=\"19.78\" x2=\"5.64\" y2=\"18.36\"></line>\n",
        "              <line x1=\"18.36\" y1=\"5.64\" x2=\"19.78\" y2=\"4.22\"></line>\n",
        "            </svg>\n",
        "            <span id=\"theme-text\">Dark</span>\n",
        "          </button>\n",
        "        </div>\n",
        "      </div>\n",
        "      <script>\n",
        "        (function() {\n",
        "          // Get theme from localStorage or default to light\n",
        "          const currentTheme = localStorage.getItem('theme') || 'light';\n",
        "          const html = document.documentElement;\n",
        "          \n",
        "          // Set initial theme\n",
        "          html.setAttribute('data-theme', currentTheme);\n",
        "          \n",
        "          // Theme toggle function\n",
        "          function toggleTheme() {\n",
        "            const currentTheme = html.getAttribute('data-theme');\n",
        "            const newTheme = currentTheme === 'light' ? 'dark' : 'light';\n",
        "            \n",
        "            html.setAttribute('data-theme', newTheme);\n",
        "            localStorage.setItem('theme', newTheme);\n",
        "            \n",
        "            // Update button text and icon\n",
        "            updateThemeButton(newTheme);\n",
        "          }\n",
        "          \n",
        "          // Update button appearance\n",
        "          function updateThemeButton(theme) {\n",
        "            const themeText = document.getElementById('theme-text');\n",
        "            const themeIcon = document.getElementById('theme-icon');\n",
        "            \n",
        "            if (theme === 'dark') {\n",
        "              themeText.textContent = 'Light';\n",
        "              // Moon icon for dark mode (switch to light)\n",
        "              themeIcon.innerHTML = '<path d=\"M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z\"></path>';\n",
        "            } else {\n",
        "              themeText.textContent = 'Dark';\n",
        "              // Sun icon for light mode (switch to dark)\n",
        "              themeIcon.innerHTML = '<circle cx=\"12\" cy=\"12\" r=\"5\"></circle><line x1=\"12\" y1=\"1\" x2=\"12\" y2=\"3\"></line><line x1=\"12\" y1=\"21\" x2=\"12\" y2=\"23\"></line><line x1=\"4.22\" y1=\"4.22\" x2=\"5.64\" y2=\"5.64\"></line><line x1=\"18.36\" y1=\"18.36\" x2=\"19.78\" y2=\"19.78\"></line><line x1=\"1\" y1=\"12\" x2=\"3\" y2=\"12\"></line><line x1=\"21\" y1=\"12\" x2=\"23\" y2=\"12\"></line><line x1=\"4.22\" y1=\"19.78\" x2=\"5.64\" y2=\"18.36\"></line><line x1=\"18.36\" y1=\"5.64\" x2=\"19.78\" y2=\"4.22\"></line>';\n",
        "            }\n",
        "          }\n",
        "          \n",
        "          // Initialize button state\n",
        "          updateThemeButton(currentTheme);\n",
        "          \n",
        "          // Add event listener\n",
        "          const themeToggle = document.getElementById('theme-toggle');\n",
        "          if (themeToggle) {\n",
        "            themeToggle.addEventListener('click', toggleTheme);\n",
        "          }\n",
        "        })();\n",
        "      </script>\n",
        "    include-after-body: |\n",
        "      <div class=\"custom-footer\">\n",
        "        <div class=\"footer-content\">\n",
        "          <p style=\"margin: 0; font-size: 1.1em; font-weight: 600;\">Random Forest Algorithms: Applications to Large-Scale Datasets</p>\n",
        "          <div class=\"footer-links\">\n",
        "            <a href=\"https://github.com/Exios66\" target=\"_blank\" rel=\"noopener\">\n",
        "              <svg width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"currentColor\" style=\"vertical-align: middle; margin-right: 5px;\">\n",
        "                <path d=\"M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z\"/>\n",
        "              </svg>\n",
        "              Visit GitHub Profile: Exios66\n",
        "            </a>\n",
        "          </div>\n",
        "          <p style=\"margin: 15px 0 0 0; font-size: 0.9em; opacity: 0.9;\">© 2024 Jack J. Burleson | Generated with Quarto</p>\n",
        "        </div>\n",
        "      </div>\n",
        "editor: visual\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## Abstract\n",
        "\n",
        "Random Forest algorithms represent one of the most robust and widely-applied machine learning techniques for both classification and regression tasks. This document provides a comprehensive examination of Random Forest methodology, its theoretical foundations, practical implementation strategies, and applications to large-scale datasets. Through empirical demonstrations and code examples, we illustrate the effectiveness of ensemble learning approaches in handling complex, high-dimensional data structures while maintaining interpretability and computational efficiency.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Random Forest, introduced by Breiman (2001), is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of classes (classification) or mean prediction (regression) of the individual trees. The algorithm's strength lies in its ability to reduce overfitting through the aggregation of diverse tree predictions while maintaining high predictive accuracy across diverse domains.\n",
        "\n",
        "### Key Advantages\n",
        "\n",
        "- **Robustness to Overfitting**: Through bootstrap aggregation and random feature selection\n",
        "- **Handling of High-Dimensional Data**: Effective feature selection mechanisms\n",
        "- **Non-parametric Nature**: No assumptions about data distribution\n",
        "- **Feature Importance**: Built-in mechanisms for variable importance assessment\n",
        "- **Scalability**: Efficient parallelization capabilities\n",
        "\n",
        "## Theoretical Foundations\n",
        "\n",
        "### Bootstrap Aggregation (Bagging)\n",
        "\n",
        "Random Forest employs bootstrap aggregation, where each tree is trained on a random subset of the training data sampled with replacement. This process reduces variance and improves generalization performance.\n",
        "\n",
        "### Random Feature Selection\n",
        "\n",
        "At each node split, the algorithm considers only a random subset of features, typically $\\sqrt{p}$ for classification and $p/3$ for regression, where $p$ is the total number of features. This decorrelates the trees and enhances model diversity.\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "For a Random Forest with $B$ trees, the prediction for a new observation $\\mathbf{x}$ is:\n",
        "\n",
        "$$\\hat{f}_{RF}(\\mathbf{x}) = \\frac{1}{B}\\sum_{b=1}^{B} T_b(\\mathbf{x})$$\n",
        "\n",
        "where $T_b(\\mathbf{x})$ represents the prediction from the $b$-th tree.\n",
        "\n",
        "## Implementation: Core Components\n",
        "\n",
        "### Essential Libraries and Setup"
      ],
      "id": "4384c1b1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup\n",
        "#| include: true\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import (accuracy_score, classification_report, \n",
        "                            confusion_matrix, mean_squared_error, r2_score)\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ],
      "id": "setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation for Large Datasets"
      ],
      "id": "5feabdee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: data-prep\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "def prepare_large_dataset(n_samples=10000, n_features=50, n_informative=20):\n",
        "    \"\"\"\n",
        "    Generate a synthetic large-scale dataset for demonstration.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_samples : int\n",
        "        Number of samples\n",
        "    n_features : int\n",
        "        Total number of features\n",
        "    n_informative : int\n",
        "        Number of informative features\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    X, y : tuple\n",
        "        Feature matrix and target vector\n",
        "    \"\"\"\n",
        "    X, y = make_classification(\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features,\n",
        "        n_informative=n_informative,\n",
        "        n_redundant=n_features - n_informative,\n",
        "        n_clusters_per_class=1,\n",
        "        random_state=42\n",
        "    )\n",
        "    return X, y\n",
        "\n",
        "# Generate large dataset\n",
        "X, y = prepare_large_dataset(n_samples=50000, n_features=100, n_informative=40)\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Target distribution: {np.bincount(y)}\")"
      ],
      "id": "data-prep",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest Classifier: Critical Implementation"
      ],
      "id": "e5230bba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: rf-classifier\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize Random Forest with optimized hyperparameters\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,           # Number of trees\n",
        "    max_depth=None,              # Maximum depth (None = unlimited)\n",
        "    min_samples_split=2,         # Minimum samples to split node\n",
        "    min_samples_leaf=1,          # Minimum samples in leaf node\n",
        "    max_features='sqrt',         # Number of features to consider (sqrt for classification)\n",
        "    bootstrap=True,              # Bootstrap sampling\n",
        "    oob_score=True,              # Out-of-bag score\n",
        "    random_state=42,\n",
        "    n_jobs=-1                    # Use all available cores\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate performance\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "oob_score = rf_classifier.oob_score_\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Out-of-Bag Score: {oob_score:.4f}\")"
      ],
      "id": "rf-classifier",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Importance Analysis"
      ],
      "id": "e9bbc322"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-height": 8,
        "fig-width": 10
      },
      "source": [
        "#| label: feature-importance\n",
        "#| echo: true\n",
        "#| fig-cap: Feature Importance Rankings from Random Forest Model\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "indices = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "# Visualize top 20 most important features\n",
        "top_n = 20\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(range(top_n), feature_importances[indices[:top_n]])\n",
        "plt.yticks(range(top_n), [f'Feature {i}' for i in indices[:top_n]])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.title('Top 20 Feature Importances')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "feature-importance",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Tuning for Large Datasets"
      ],
      "id": "4efc0e6f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: hyperparameter-tuning\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Define parameter grid for optimization\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Use a subset for faster grid search on large datasets\n",
        "X_train_subset = X_train[:10000]  # Sample for grid search\n",
        "y_train_subset = y_train[:10000]\n",
        "\n",
        "# Grid search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "    param_grid,\n",
        "    cv=3,                          # 3-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Score:\", grid_search.best_score_)"
      ],
      "id": "hyperparameter-tuning",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest Regressor: Regression Applications"
      ],
      "id": "a002d944"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: rf-regressor\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Generate regression dataset\n",
        "X_reg, y_reg = make_regression(\n",
        "    n_samples=10000,\n",
        "    n_features=50,\n",
        "    n_informative=30,\n",
        "    noise=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    max_features=None,  # None uses all features (default for regression)\n",
        "    bootstrap=True,\n",
        "    oob_score=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred_reg = rf_regressor.predict(X_test_reg)\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "print(f\"Out-of-Bag Score: {rf_regressor.oob_score_:.4f}\")"
      ],
      "id": "rf-regressor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling Imbalanced Datasets"
      ],
      "id": "e726cd9a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: imbalanced-data\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X_imb, y_imb = make_classification(\n",
        "    n_samples=10000,\n",
        "    n_features=30,\n",
        "    n_informative=15,\n",
        "    weights=[0.9, 0.1],  # 90% class 0, 10% class 1\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
        "    X_imb, y_imb, test_size=0.2, random_state=42, stratify=y_imb\n",
        ")\n",
        "\n",
        "# Random Forest with class_weight='balanced'\n",
        "rf_balanced = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    class_weight='balanced',  # Automatically adjust class weights\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_balanced.fit(X_train_imb, y_train_imb)\n",
        "y_pred_imb = rf_balanced.predict(X_test_imb)\n",
        "\n",
        "print(\"Classification Report (Balanced Random Forest):\")\n",
        "print(classification_report(y_test_imb, y_pred_imb))"
      ],
      "id": "imbalanced-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Optimization for Large Datasets\n",
        "\n",
        "### Incremental Learning and Memory Efficiency"
      ],
      "id": "d0930a21"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: memory-optimization\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# For very large datasets, consider these strategies:\n",
        "# 1. Use max_samples parameter to limit bootstrap sample size\n",
        "# 2. Set max_depth to prevent excessive memory usage\n",
        "# 3. Use warm_start for incremental training\n",
        "\n",
        "rf_efficient = RandomForestClassifier(\n",
        "    n_estimators=50,\n",
        "    max_depth=15,              # Limit tree depth\n",
        "    max_samples=0.5,           # Use 50% of data for each tree\n",
        "    max_features='sqrt',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train on large dataset\n",
        "rf_efficient.fit(X_train, y_train)\n",
        "print(f\"Model trained successfully on {X_train.shape[0]} samples\")\n",
        "print(f\"Memory-efficient configuration completed\")"
      ],
      "id": "memory-optimization",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parallel Processing Configuration"
      ],
      "id": "333f334b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: parallel-processing\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "import time\n",
        "\n",
        "# Compare sequential vs parallel processing\n",
        "X_sample = X_train[:5000]\n",
        "y_sample = y_train[:5000]\n",
        "\n",
        "# Sequential processing\n",
        "start_time = time.time()\n",
        "rf_seq = RandomForestClassifier(n_estimators=100, n_jobs=1, random_state=42)\n",
        "rf_seq.fit(X_sample, y_sample)\n",
        "seq_time = time.time() - start_time\n",
        "\n",
        "# Parallel processing\n",
        "start_time = time.time()\n",
        "rf_par = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "rf_par.fit(X_sample, y_sample)\n",
        "par_time = time.time() - start_time\n",
        "\n",
        "print(f\"Sequential time: {seq_time:.2f} seconds\")\n",
        "print(f\"Parallel time: {par_time:.2f} seconds\")\n",
        "print(f\"Speedup: {seq_time/par_time:.2f}x\")"
      ],
      "id": "parallel-processing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets for Random Forest Applications\n",
        "\n",
        "### Publicly Available Large-Scale Datasets\n",
        "\n",
        "The following datasets are excellent for demonstrating Random Forest algorithms on large-scale problems:\n",
        "\n",
        "#### Classification Datasets\n",
        "\n",
        "1. **UCI Machine Learning Repository**\n",
        "   - URL: https://archive.ics.uci.edu/\n",
        "   - Notable datasets: Covertype, KDD Cup 1999, Adult Census\n",
        "   - Access: Direct download via Python `sklearn.datasets` or manual download\n",
        "\n",
        "2. **Kaggle Datasets**\n",
        "   - URL: https://www.kaggle.com/datasets\n",
        "   - Large-scale competitions: Titanic, House Prices, Credit Card Fraud Detection\n",
        "   - Access: Requires Kaggle account and API key\n",
        "\n",
        "3. **OpenML**\n",
        "   - URL: https://www.openml.org/\n",
        "   - Access via Python: `from openml import datasets`"
      ],
      "id": "0a504e28"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: dataset-access\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Example: Loading UCI datasets via scikit-learn\n",
        "try:\n",
        "    from sklearn.datasets import fetch_covtype\n",
        "    covtype = fetch_covtype()\n",
        "    print(f\"Covertype dataset shape: {covtype.data.shape}\")\n",
        "    print(f\"Number of classes: {len(np.unique(covtype.target))}\")\n",
        "except:\n",
        "    print(\"Dataset fetch requires internet connection\")\n",
        "\n",
        "# Example: Using OpenML\n",
        "try:\n",
        "    from openml.datasets import get_dataset\n",
        "    # dataset = get_dataset(1)  # Uncomment to fetch specific dataset\n",
        "    print(\"OpenML integration available\")\n",
        "except ImportError:\n",
        "    print(\"Install openml: pip install openml\")"
      ],
      "id": "dataset-access",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Regression Datasets\n",
        "\n",
        "1. **California Housing Dataset** (Built-in scikit-learn)\n",
        "2. **Boston Housing Dataset** (Historical, now deprecated)\n",
        "3. **Ames Housing Dataset** (Kaggle)"
      ],
      "id": "1d1d4b7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: regression-datasets\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "print(f\"California Housing dataset shape: {housing.data.shape}\")\n",
        "print(f\"Features: {housing.feature_names}\")"
      ],
      "id": "regression-datasets",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Loading Function"
      ],
      "id": "890fb6b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: dataset-loader\n",
        "#| echo: true\n",
        "\n",
        "def load_dataset(dataset_name='synthetic', n_samples=10000):\n",
        "    \"\"\"\n",
        "    Unified dataset loading function.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    dataset_name : str\n",
        "        Name of dataset ('synthetic', 'california_housing', etc.)\n",
        "    n_samples : int\n",
        "        For synthetic datasets\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    X, y : tuple\n",
        "        Feature matrix and target\n",
        "    \"\"\"\n",
        "    if dataset_name == 'synthetic':\n",
        "        X, y = make_classification(\n",
        "            n_samples=n_samples,\n",
        "            n_features=50,\n",
        "            n_informative=20,\n",
        "            random_state=42\n",
        "        )\n",
        "    elif dataset_name == 'california_housing':\n",
        "        data = fetch_california_housing()\n",
        "        X, y = data.data, data.target\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "    \n",
        "    return X, y"
      ],
      "id": "dataset-loader",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Machine Learning Resources\n",
        "\n",
        "### Online Courses and Tutorials\n",
        "\n",
        "1. **Coursera - Machine Learning Specialization**\n",
        "   - URL: https://www.coursera.org/learn/machine-learning\n",
        "   - Covers ensemble methods including Random Forests\n",
        "\n",
        "2. **edX - MIT Introduction to Machine Learning**\n",
        "   - URL: https://www.edx.org/course/introduction-to-machine-learning\n",
        "   - Comprehensive coverage of ML fundamentals\n",
        "\n",
        "3. **Fast.ai - Practical Deep Learning**\n",
        "   - URL: https://www.fast.ai/\n",
        "   - Modern approach to ML with practical applications\n",
        "\n",
        "### Books and Textbooks\n",
        "\n",
        "1. **\"The Elements of Statistical Learning\"** by Hastie, Tibshirani, and Friedman\n",
        "   - Comprehensive treatment of statistical learning methods\n",
        "   - Chapter 15 covers Random Forests in detail\n",
        "\n",
        "2. **\"An Introduction to Statistical Learning\"** by James et al.\n",
        "   - More accessible introduction to ML concepts\n",
        "   - Excellent for beginners\n",
        "\n",
        "3. **\"Hands-On Machine Learning\"** by Aurélien Géron\n",
        "   - Practical implementation focus\n",
        "   - Code examples in Python\n",
        "\n",
        "### Software and Libraries\n",
        "\n",
        "1. **scikit-learn** (Python)\n",
        "   - Primary library used in this document\n",
        "   - Documentation: https://scikit-learn.org/stable/\n",
        "\n",
        "2. **XGBoost** (Gradient Boosting)\n",
        "   - Advanced ensemble method\n",
        "   - URL: https://xgboost.readthedocs.io/\n",
        "\n",
        "3. **LightGBM** (Microsoft)\n",
        "   - Fast gradient boosting framework\n",
        "   - URL: https://lightgbm.readthedocs.io/\n",
        "\n",
        "4. **R - randomForest package**\n",
        "   - Original implementation by Breiman\n",
        "   - Comprehensive R documentation\n",
        "\n",
        "### Research Communities\n",
        "\n",
        "1. **Papers with Code**\n",
        "   - URL: https://paperswithcode.com/\n",
        "   - Latest research papers with implementations\n",
        "\n",
        "2. **arXiv Machine Learning**\n",
        "   - URL: https://arxiv.org/list/cs.LG/recent\n",
        "   - Preprint repository for ML research\n",
        "\n",
        "3. **Google Scholar**\n",
        "   - Search for \"Random Forest\" and related terms\n",
        "   - Track citations and related work\n",
        "\n",
        "## Literature Review\n",
        "\n",
        "### Foundational Papers\n",
        "\n",
        "1. **Breiman, L. (2001). \"Random Forests.\" *Machine Learning*, 45(1), 5-32.**\n",
        "   - Original paper introducing Random Forest algorithm\n",
        "   - Theoretical foundations and empirical evaluations\n",
        "   - DOI: 10.1023/A:1010933404324\n",
        "\n",
        "2. **Ho, T. K. (1995). \"Random Decision Forests.\" *Proceedings of the 3rd International Conference on Document Analysis and Recognition*, 278-282.**\n",
        "   - Early work on random decision forests\n",
        "   - Precursor to Breiman's Random Forest\n",
        "\n",
        "3. **Breiman, L. (1996). \"Bagging Predictors.\" *Machine Learning*, 24(2), 123-140.**\n",
        "   - Introduction to bootstrap aggregation\n",
        "   - Foundation for ensemble methods\n",
        "   - DOI: 10.1023/A:1018054314350\n",
        "\n",
        "### Recent Advances and Applications\n",
        "\n",
        "4. **Biau, G., & Scornet, E. (2016). \"A Random Forest Guided Tour.\" *Test*, 25(2), 197-227.**\n",
        "   - Comprehensive review of Random Forest theory\n",
        "   - Statistical properties and consistency results\n",
        "   - DOI: 10.1007/s11749-016-0481-7\n",
        "\n",
        "5. **Wright, M. N., & Ziegler, A. (2017). \"ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.\" *Journal of Statistical Software*, 77(1), 1-17.**\n",
        "   - Efficient implementation for high-dimensional data\n",
        "   - Performance optimizations\n",
        "   - DOI: 10.18637/jss.v077.i01\n",
        "\n",
        "6. **Probst, P., Boulesteix, A. L., & Bischl, B. (2019). \"Tunability: Importance of Hyperparameters of Machine Learning Algorithms.\" *Journal of Machine Learning Research*, 20(53), 1-32.**\n",
        "   - Analysis of hyperparameter importance\n",
        "   - Includes Random Forest hyperparameter sensitivity\n",
        "   - URL: http://jmlr.org/papers/v20/18-444.html\n",
        "\n",
        "### Large-Scale Applications\n",
        "\n",
        "7. **Caruana, R., & Niculescu-Mizil, A. (2006). \"An Empirical Comparison of Supervised Learning Algorithms.\" *Proceedings of the 23rd International Conference on Machine Learning*, 161-168.**\n",
        "   - Comparative study including Random Forests\n",
        "   - Performance across multiple datasets\n",
        "\n",
        "8. **Fernández-Delgado, M., et al. (2014). \"Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?\" *Journal of Machine Learning Research*, 15, 3133-3181.**\n",
        "   - Large-scale comparison of classifiers\n",
        "   - Random Forest performance analysis\n",
        "\n",
        "### Feature Importance and Interpretability\n",
        "\n",
        "9. **Strobl, C., et al. (2007). \"Bias in Random Forest Variable Importance Measures: Illustrations, Sources and a Solution.\" *BMC Bioinformatics*, 8(1), 25.**\n",
        "   - Critical analysis of variable importance measures\n",
        "   - Solutions for categorical variables\n",
        "   - DOI: 10.1186/1471-2105-8-25\n",
        "\n",
        "10. **Lundberg, S. M., & Lee, S. I. (2017). \"A Unified Approach to Interpreting Model Predictions.\" *Advances in Neural Information Processing Systems*, 30.**\n",
        "    - SHAP values for model interpretation\n",
        "    - Applicable to Random Forest models\n",
        "\n",
        "### Theoretical Developments\n",
        "\n",
        "11. **Scornet, E., Biau, G., & Vert, J. P. (2015). \"Consistency of Random Forests.\" *The Annals of Statistics*, 43(4), 1716-1741.**\n",
        "    - Theoretical consistency results\n",
        "    - Asymptotic properties\n",
        "    - DOI: 10.1214/15-AOS1321\n",
        "\n",
        "12. **Mentch, L., & Hooker, G. (2016). \"Quantifying Uncertainty in Random Forests via Confidence Intervals and Hypothesis Tests.\" *Journal of Machine Learning Research*, 17(26), 1-41.**\n",
        "    - Statistical inference for Random Forests\n",
        "    - Confidence intervals and hypothesis testing\n",
        "\n",
        "## Gradient Boosting Methods\n",
        "\n",
        "Gradient Boosting represents another powerful ensemble learning paradigm that sequentially builds models to correct errors from previous iterations. Unlike Random Forests, which use parallel tree construction, gradient boosting employs sequential, additive modeling to minimize prediction errors.\n",
        "\n",
        "### Theoretical Foundations\n",
        "\n",
        "Gradient Boosting iteratively fits weak learners (typically decision trees) to the negative gradient of a loss function. The algorithm minimizes:\n",
        "\n",
        "$$L(y, F(x)) = \\sum_{i=1}^{n} L(y_i, F(x_i))$$\n",
        "\n",
        "where $F(x) = \\sum_{m=1}^{M} \\gamma_m h_m(x)$ represents the additive model, and $h_m(x)$ are the base learners.\n",
        "\n",
        "### XGBoost: Extreme Gradient Boosting\n",
        "\n",
        "XGBoost extends gradient boosting with regularization, parallel processing, and efficient tree construction algorithms."
      ],
      "id": "7ed63b85"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: xgboost-implementation\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "    \n",
        "    # Prepare data\n",
        "    X_train_gb, X_test_gb, y_train_gb, y_test_gb = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    # XGBoost Classifier\n",
        "    xgb_classifier = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        tree_method='hist'  # Efficient histogram-based algorithm\n",
        "    )\n",
        "    \n",
        "    xgb_classifier.fit(X_train_gb, y_train_gb)\n",
        "    y_pred_xgb = xgb_classifier.predict(X_test_gb)\n",
        "    xgb_accuracy = accuracy_score(y_test_gb, y_pred_xgb)\n",
        "    \n",
        "    print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
        "    print(f\"Feature Importance (Top 5):\")\n",
        "    feature_imp = xgb_classifier.feature_importances_\n",
        "    top_indices = np.argsort(feature_imp)[::-1][:5]\n",
        "    for idx in top_indices:\n",
        "        print(f\"  Feature {idx}: {feature_imp[idx]:.4f}\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"XGBoost not installed. Install with: pip install xgboost\")"
      ],
      "id": "xgboost-implementation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LightGBM: Light Gradient Boosting Machine\n",
        "\n",
        "LightGBM uses gradient-based one-side sampling (GOSS) and exclusive feature bundling (EFB) for improved efficiency on large datasets."
      ],
      "id": "61c9a637"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightgbm-implementation\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    \n",
        "    # LightGBM Classifier\n",
        "    lgb_classifier = lgb.LGBMClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1\n",
        "    )\n",
        "    \n",
        "    lgb_classifier.fit(X_train_gb, y_train_gb)\n",
        "    y_pred_lgb = lgb_classifier.predict(X_test_gb)\n",
        "    lgb_accuracy = accuracy_score(y_test_gb, y_pred_lgb)\n",
        "    \n",
        "    print(f\"LightGBM Accuracy: {lgb_accuracy:.4f}\")\n",
        "    print(f\"Training time advantage: LightGBM is typically faster than XGBoost\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"LightGBM not installed. Install with: pip install lightgbm\")"
      ],
      "id": "lightgbm-implementation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison: Random Forest vs Gradient Boosting"
      ],
      "id": "c0d6594f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: ensemble-comparison\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "import time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Prepare smaller subset for fair comparison\n",
        "X_comp = X_train_gb[:10000]\n",
        "y_comp = y_train_gb[:10000]\n",
        "\n",
        "# Random Forest\n",
        "start = time.time()\n",
        "rf_comp = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "rf_comp.fit(X_comp, y_comp)\n",
        "rf_time = time.time() - start\n",
        "\n",
        "# XGBoost\n",
        "try:\n",
        "    start = time.time()\n",
        "    xgb_comp = xgb.XGBClassifier(n_estimators=100, n_jobs=-1, random_state=42, tree_method='hist')\n",
        "    xgb_comp.fit(X_comp, y_comp)\n",
        "    xgb_time = time.time() - start\n",
        "    \n",
        "    print(\"Training Time Comparison (10,000 samples):\")\n",
        "    print(f\"  Random Forest: {rf_time:.2f} seconds\")\n",
        "    print(f\"  XGBoost: {xgb_time:.2f} seconds\")\n",
        "    print(f\"  Speedup: {rf_time/xgb_time:.2f}x\")\n",
        "except:\n",
        "    print(\"XGBoost comparison requires xgboost installation\")"
      ],
      "id": "ensemble-comparison",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Support Vector Machines for Large-Scale Data\n",
        "\n",
        "Support Vector Machines (SVMs) are powerful for classification and regression, but traditional implementations scale poorly with large datasets. Modern approaches use kernel approximations and linear SVMs for scalability.\n",
        "\n",
        "### Linear SVM with Stochastic Gradient Descent"
      ],
      "id": "cbe5dcea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: linear-svm\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Linear SVM using SGD (scales to millions of samples)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_gb)\n",
        "X_test_scaled = scaler.transform(X_test_gb)\n",
        "\n",
        "svm_sgd = SGDClassifier(\n",
        "    loss='hinge',           # SVM loss\n",
        "    alpha=0.0001,            # Regularization parameter\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "svm_sgd.fit(X_train_scaled, y_train_gb)\n",
        "y_pred_svm = svm_sgd.predict(X_test_scaled)\n",
        "svm_accuracy = accuracy_score(y_test_gb, y_pred_svm)\n",
        "\n",
        "print(f\"Linear SVM (SGD) Accuracy: {svm_accuracy:.4f}\")\n",
        "print(f\"Memory efficient: Suitable for datasets with millions of samples\")"
      ],
      "id": "linear-svm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Kernel Approximation for Non-Linear SVMs"
      ],
      "id": "1bfe7bf6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: kernel-approximation\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "from sklearn.kernel_approximation import RBFSampler, Nystroem\n",
        "\n",
        "# RBF Kernel approximation\n",
        "rbf_sampler = RBFSampler(\n",
        "    gamma=0.1,\n",
        "    n_components=100,  # Number of random features\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train_rbf = rbf_sampler.fit_transform(X_train_scaled)\n",
        "X_test_rbf = rbf_sampler.transform(X_test_scaled)\n",
        "\n",
        "svm_rbf = SGDClassifier(loss='hinge', alpha=0.0001, random_state=42)\n",
        "svm_rbf.fit(X_train_rbf, y_train_gb)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_rbf)\n",
        "rbf_accuracy = accuracy_score(y_test_gb, y_pred_rbf)\n",
        "\n",
        "print(f\"RBF Kernel Approximation Accuracy: {rbf_accuracy:.4f}\")\n",
        "print(f\"Enables non-linear SVMs on large datasets\")"
      ],
      "id": "kernel-approximation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Networks for Large-Scale Learning\n",
        "\n",
        "Deep learning approaches, particularly feedforward neural networks, can effectively handle large datasets when properly configured with regularization and efficient optimization.\n",
        "\n",
        "### Multi-Layer Perceptron with scikit-learn"
      ],
      "id": "d58e6d3d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: mlp-classifier\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Multi-layer Perceptron\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),  # Two hidden layers\n",
        "    activation='relu',\n",
        "    solver='adam',                # Efficient for large datasets\n",
        "    alpha=0.0001,                 # L2 regularization\n",
        "    batch_size=200,               # Mini-batch size\n",
        "    learning_rate='adaptive',\n",
        "    max_iter=500,\n",
        "    random_state=42,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1\n",
        ")\n",
        "\n",
        "mlp.fit(X_train_scaled, y_train_gb)\n",
        "y_pred_mlp = mlp.predict(X_test_scaled)\n",
        "mlp_accuracy = accuracy_score(y_test_gb, y_pred_mlp)\n",
        "\n",
        "print(f\"MLP Accuracy: {mlp_accuracy:.4f}\")\n",
        "print(f\"Architecture: {mlp.hidden_layer_sizes}\")\n",
        "print(f\"Number of iterations: {mlp.n_iter_}\")"
      ],
      "id": "mlp-classifier",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep Learning with TensorFlow/Keras (Optional)"
      ],
      "id": "8236f9d6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: deep-learning\n",
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Uncomment to use TensorFlow/Keras\n",
        "\"\"\"\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    \n",
        "    # Build neural network\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        X_train_scaled, y_train_gb,\n",
        "        epochs=50,\n",
        "        batch_size=256,\n",
        "        validation_split=0.2,\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    # Evaluate\n",
        "    test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_gb, verbose=0)\n",
        "    print(f\"Deep Neural Network Accuracy: {test_accuracy:.4f}\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"TensorFlow not installed. Install with: pip install tensorflow\")\n",
        "\"\"\""
      ],
      "id": "deep-learning",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regularized Linear Models\n",
        "\n",
        "Linear models with regularization (Ridge, Lasso, Elastic Net) are highly scalable and provide interpretable results, making them excellent baselines for large-scale problems.\n",
        "\n",
        "### Ridge Regression"
      ],
      "id": "39c64902"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: ridge-regression\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "from sklearn.linear_model import Ridge, RidgeClassifier\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Ridge Regression for continuous targets\n",
        "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "ridge_reg = Ridge(alpha=1.0, solver='sag', random_state=42)  # SAG for large datasets\n",
        "ridge_reg.fit(X_reg_train, y_reg_train)\n",
        "y_pred_ridge = ridge_reg.predict(X_reg_test)\n",
        "ridge_r2 = r2_score(y_reg_test, y_pred_ridge)\n",
        "\n",
        "print(f\"Ridge Regression R²: {ridge_r2:.4f}\")\n",
        "print(f\"Highly scalable to millions of samples\")"
      ],
      "id": "ridge-regression",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Elastic Net: Combining L1 and L2 Regularization"
      ],
      "id": "e9d22299"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: elastic-net\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
        "\n",
        "# Elastic Net with cross-validation for hyperparameter tuning\n",
        "elastic_net = ElasticNetCV(\n",
        "    l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1.0],\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "elastic_net.fit(X_reg_train, y_reg_train)\n",
        "y_pred_en = elastic_net.predict(X_reg_test)\n",
        "en_r2 = r2_score(y_reg_test, y_pred_en)\n",
        "\n",
        "print(f\"Elastic Net R²: {en_r2:.4f}\")\n",
        "print(f\"Optimal L1 ratio: {elastic_net.l1_ratio_:.4f}\")\n",
        "print(f\"Combines benefits of Ridge and Lasso regularization\")"
      ],
      "id": "elastic-net",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lasso for Feature Selection"
      ],
      "id": "50958f36"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lasso-feature-selection\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "from sklearn.linear_model import Lasso, LassoCV\n",
        "\n",
        "# Lasso with cross-validation\n",
        "lasso = LassoCV(cv=5, n_jobs=-1, random_state=42, max_iter=2000)\n",
        "lasso.fit(X_reg_train, y_reg_train)\n",
        "\n",
        "# Count non-zero coefficients (selected features)\n",
        "n_selected = np.sum(lasso.coef_ != 0)\n",
        "total_features = len(lasso.coef_)\n",
        "\n",
        "y_pred_lasso = lasso.predict(X_reg_test)\n",
        "lasso_r2 = r2_score(y_reg_test, y_pred_lasso)\n",
        "\n",
        "print(f\"Lasso R²: {lasso_r2:.4f}\")\n",
        "print(f\"Selected features: {n_selected}/{total_features} ({100*n_selected/total_features:.1f}%)\")\n",
        "print(f\"Automatic feature selection through L1 regularization\")"
      ],
      "id": "lasso-feature-selection",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparative Analysis of ML Techniques\n",
        "\n",
        "### Performance Comparison on Large Datasets"
      ],
      "id": "6de05b2b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-height": 6,
        "fig-width": 10
      },
      "source": [
        "#| label: ml-comparison\n",
        "#| echo: true\n",
        "#| fig-cap: Comparison of ML Techniques on Large-Scale Dataset\n",
        "\n",
        "# Compare multiple algorithms\n",
        "algorithms = {}\n",
        "results = {}\n",
        "\n",
        "# Random Forest\n",
        "rf_comp = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "rf_comp.fit(X_train_gb[:20000], y_train_gb[:20000])\n",
        "results['Random Forest'] = accuracy_score(y_test_gb, rf_comp.predict(X_test_gb))\n",
        "\n",
        "# XGBoost\n",
        "try:\n",
        "    xgb_comp = xgb.XGBClassifier(n_estimators=100, n_jobs=-1, random_state=42, tree_method='hist')\n",
        "    xgb_comp.fit(X_train_gb[:20000], y_train_gb[:20000])\n",
        "    results['XGBoost'] = accuracy_score(y_test_gb, xgb_comp.predict(X_test_gb))\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Linear SVM\n",
        "results['Linear SVM'] = svm_accuracy\n",
        "\n",
        "# MLP\n",
        "results['MLP'] = mlp_accuracy\n",
        "\n",
        "# Visualize comparison\n",
        "if results:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    algorithms = list(results.keys())\n",
        "    accuracies = list(results.values())\n",
        "    colors = ['#667eea', '#764ba2', '#f093fb', '#4facfe', '#00f2fe']\n",
        "    \n",
        "    bars = plt.bar(algorithms, accuracies, color=colors[:len(algorithms)])\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Machine Learning Algorithm Comparison')\n",
        "    plt.ylim([min(accuracies) - 0.05, max(accuracies) + 0.05])\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{acc:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "ml-comparison",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When to Use Each Technique\n",
        "\n",
        "### Decision Guide\n",
        "\n",
        "| Technique | Best For | Strengths | Limitations |\n",
        "|-----------|----------|-----------|-------------|\n",
        "| **Random Forest** | Baseline, feature importance, interpretability | Robust, parallelizable, handles mixed data types | Memory intensive, slower than gradient boosting |\n",
        "| **XGBoost/LightGBM** | High accuracy, competitions, structured data | State-of-the-art performance, efficient | More hyperparameters, less interpretable |\n",
        "| **Linear SVM** | Text classification, high-dimensional sparse data | Memory efficient, fast training | Limited to linear or approximated kernels |\n",
        "| **Neural Networks** | Complex patterns, unstructured data | Flexible, can learn non-linear relationships | Requires more data, longer training time |\n",
        "| **Regularized Linear Models** | Baseline, interpretability, feature selection | Fast, interpretable, scalable | Limited to linear relationships |\n",
        "\n",
        "### Scalability Considerations"
      ],
      "id": "4265fa29"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: scalability-analysis\n",
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "print(\"Scalability Analysis for Large Datasets:\")\n",
        "print(\"\\n1. Random Forest:\")\n",
        "print(\"   - Memory: O(n × m × trees)\")\n",
        "print(\"   - Training: O(n × m × log(m) × trees)\")\n",
        "print(\"   - Parallelizable: Yes\")\n",
        "\n",
        "print(\"\\n2. Gradient Boosting (XGBoost/LightGBM):\")\n",
        "print(\"   - Memory: O(n × m)\")\n",
        "print(\"   - Training: O(n × m × log(m) × iterations)\")\n",
        "print(\"   - Parallelizable: Yes (tree-level)\")\n",
        "\n",
        "print(\"\\n3. Linear SVM (SGD):\")\n",
        "print(\"   - Memory: O(m)\")\n",
        "print(\"   - Training: O(n × m × iterations)\")\n",
        "print(\"   - Parallelizable: Yes (sample-level)\")\n",
        "\n",
        "print(\"\\n4. Neural Networks:\")\n",
        "print(\"   - Memory: O(parameters)\")\n",
        "print(\"   - Training: O(n × parameters × epochs)\")\n",
        "print(\"   - Parallelizable: Yes (GPU acceleration)\")\n",
        "\n",
        "print(\"\\n5. Regularized Linear Models:\")\n",
        "print(\"   - Memory: O(m)\")\n",
        "print(\"   - Training: O(n × m × iterations)\")\n",
        "print(\"   - Parallelizable: Yes\")"
      ],
      "id": "scalability-analysis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Random Forest algorithms represent a powerful and versatile tool in the machine learning practitioner's toolkit. Through bootstrap aggregation and random feature selection, these ensemble methods achieve robust performance across diverse application domains while maintaining computational efficiency and interpretability through feature importance measures.\n",
        "\n",
        "The demonstrated implementations illustrate key aspects of Random Forest methodology, including hyperparameter optimization, handling of large-scale datasets, and performance evaluation. As machine learning continues to evolve, Random Forests remain a reliable baseline method, particularly valuable for their ability to handle high-dimensional data, provide feature importance insights, and deliver strong predictive performance with minimal preprocessing requirements.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- Random Forests excel at handling large, high-dimensional datasets\n",
        "- Feature importance measures provide valuable interpretability\n",
        "- Proper hyperparameter tuning significantly impacts performance\n",
        "- Parallel processing enables efficient training on large datasets\n",
        "- The method serves as an excellent baseline for comparison with more complex models\n",
        "\n",
        "## References\n",
        "\n",
        "All cited literature is listed in the Literature Review section above. For comprehensive bibliographic information, readers are encouraged to consult the original publications through their respective digital object identifiers (DOIs) or publication URLs.\n",
        "\n",
        "---\n",
        "\n",
        "*This document was generated using Quarto. For reproducible research, all code chunks can be executed to regenerate the analyses and visualizations presented herein.*"
      ],
      "id": "64e6ba1b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}